{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name: Haozheng Liu** \\\n",
    "**Email: hl6602@nyu.edu**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer:**

1. To make sure the model generalizes to *new speakers*, I would use a **speaker-independent split**. Instead of randomly splitting individual phone recordings, I first shuffle the 100 speakers and then assign them to three disjoint sets, for example:
- **Train:** ~70 speakers  
- **Validation:** ~15 speakers  
- **Test:** ~15 speakers  

All recordings from a given person (all 5 days × ~200 phones/day) go **only into one split**. No speaker appears in more than one set. This way, the validation and test sets consist of **entirely unseen speakers**, so good performance there indicates that the model has actually learned to generalize across speakers, not just memorize voices it has already seen.

2. When we receive 10,000 additional recordings from **Kilian**, we want the model to perform especially well for him while still generalizing. I would keep the **original speaker-independent train/val/test split for the 100 people** to monitor generalization, and then create a **separate internal split for Kilian’s data**, e.g.:
- **Kilian-train:** ~70% of his recordings  
- **Kilian-val:** ~10–15%  
- **Kilian-test:** ~15–20% held out and never used in training  

The new **training set** will contain:
- All recordings from the original training speakers, plus  
- **Kilian-train** (optionally up-weighted so the model better adapts to his voice).  

During training/early stopping, I would monitor:
- Validation loss/accuracy on the **original validation speakers** (overall generalization), and  
- Validation performance on **Kilian-val** (speaker-specific performance).  

At the end, I report test metrics on:
- The **original test speakers** → how well the model generalizes to new people, and  
- **Kilian-test** → how well it specializes to Kilian.  

This strategy lets the model adapt to Kilian with plenty of data, while still preserving an honest evaluation of generalization to completely unseen speakers.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer:**

1. **1-NN decision boundary**

For 1-NN, each point in the plane is classified by the label of its **nearest labeled example**.  
So the decision boundary is formed by the **perpendicular bisectors** between neighboring positive and negative points (i.e., the Voronoi diagram of all 5 points, with each cell taking the label of its center).

If we plot the five points:

- Positive: (1,2), (1,4), (5,4)  
- Negative: (3,1), (3,2)  

we get three regions where any new point is classified as **positive** (closest to one of the three positive points), and two regions where a new point is **negative** (closest to one of the two negative points). The boundary between positive and negative regions lies roughly in the middle “strip” between the positive cluster on the left/top and the negative points in the center/bottom.

Any new point is classified by checking which of the five points is geometrically closest to it on the plot.

---

2. **Effect of feature scaling**

Dataset:

- Positive: (100,2), (100,4), (500,4)  
- Negative: (300,1), (300,2)  
- Test point: (500,1)

**Before scaling (raw features, Euclidean distance):**

Distances from (500,1):

- To (100,2): very large (difference of 400 in x)  
- To (100,4): very large (difference of 400 in x)  
- To (500,4): distance ≈ 3  
- To (300,1): distance ≈ 200  
- To (300,2): distance ≈ 200  

The **nearest neighbor** is (500,4), which is **positive**, so 1-NN predicts **positive** before scaling.

---

**After scaling each feature to [0,1] separately:**

- For the **x** feature, min = 100, max = 500  
- For the **y** feature, min = 1, max = 4  

After scaling, the test point (500,1) becomes approximately (1, 0). Computing distances in this scaled space:

- It is now closest to (300,1) (the negative point at x = 300), not to (500,4).  

So after proper [0,1] scaling per feature, the **nearest neighbor** becomes a **negative** example, and 1-NN predicts **negative**.

In summary:

- **Before scaling:** predicted **positive**  
- **After scaling:** predicted **negative**

---

3. **Handling missing features in K-NN**

If a test point has missing features, we can modify the distance calculation to ignore dimensions where the test point is missing. Concretely:

- When computing the distance between the test point and a training point, use only the coordinates where **both** have observed values.  
- Optionally, normalize the distance by the number of used dimensions so that points with fewer shared features are not unfairly favored or penalized.  

Another option is to **impute** missing values (e.g., using feature means, medians, or k-NN imputation) before running K-NN, but modifying the distance to skip missing dimensions is a simple and direct approach.

---

4. **Why K-NN can still work for high-dimensional images**

Even though raw images have thousands of pixels (high-dimensional feature space), image data is usually **highly structured**:

- Neighboring pixels are strongly correlated, so images lie on a much **lower-dimensional manifold** inside the high-dimensional space.
- Similar images (e.g., digits, faces, objects) tend to form **clusters**, so distances between them can still be meaningful.  
- In practice, K-NN is often applied not to raw pixels but to **feature embeddings** (e.g., from a CNN), which capture high-level structure in a lower-dimensional, more metric-friendly space.

Because of this structure and the use of good feature representations, K-NN can still find meaningful nearest neighbors and perform well on image classification, despite the large number of pixels.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer:**

1. Evaluating \(h(x) = \text{sign}(w \cdot x)\) on every \((x, y)\) in both \(D_{\text{TR}}\) and \(D_{\text{TE}}\) would, in principle, let you **estimate** the training error and test error: you just count how many predictions disagree with \(y\) in each set and divide by the number of examples. Then you can directly compare the two empirical errors and see whether the test error is higher than the training error.  
However, for the Perceptron, doing this on the **training set** is usually redundant, because we already know what the training error is (see part 2).

2. For the Perceptron algorithm (under the usual assumption that the data is linearly separable and the algorithm has converged), there is **no need** to explicitly compute training error because it is **zero by construction**.  
The Perceptron updates its weight vector only when it encounters a **misclassified training example**. When the algorithm stops, this means it has gone through the entire training set without finding any misclassified points. Therefore, the final classifier correctly labels every training example, so the training error on \(D_{\text{TR}}\) is exactly **0%**, and we do not need to recompute it afterward.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer:**

We have two training points:

- Positive: \(x^{(+)} = (10, -2), \; y = +1\)
- Negative: \(x^{(-)} = (12, 2), \; y = -1\)

Perceptron update rule (learning rate = 1):

\[
w_{t+1} = w_t + y^{(i)} x^{(i)} \quad \text{if } y^{(i)} (w_t \cdot x^{(i)}) \le 0
\]

Start with \(w_0 = (0,0)\), and visit the points in the order: positive → negative, repeating.

The updates are:

\[
\begin{aligned}
w_0 &= (0, 0) \\
w_1 &= w_0 + 1 \cdot (10, -2) = (10, -2) \\
w_2 &= w_1 + (-1)\cdot(12, 2) = (-2, -4) \\
w_3 &= w_2 + 1 \cdot (10, -2) = (8, -6) \\
w_4 &= w_3 + (-1)\cdot(12, 2) = (-4, -8) \\
w_5 &= w_4 + 1 \cdot (10, -2) = (6, -10) \\
w_6 &= w_5 + (-1)\cdot(12, 2) = (-6, -12) \\
w_7 &= w_6 + 1 \cdot (10, -2) = (4, -14) \\
w_8 &= w_7 + (-1)\cdot(12, 2) = (-8, -16) \\
w_9 &= w_8 + 1 \cdot (10, -2) = (2, -18)
\end{aligned}
\]

With \(w_9 = (2, -18)\), both points are correctly classified:

- \(w_9 \cdot (10, -2) = 56 > 0\) → positive  
- \(w_9 \cdot (12, 2) = -12 < 0\) and \(y = -1\) → negative  

So the algorithm **converges after 9 updates**, and the sequence of weight vectors is:

\[
w_0, w_1, w_2, \dots, w_9 = 
(0,0), (10,-2), (-2,-4), (8,-6), (-4,-8), (6,-10), (-6,-12), (4,-14), (-8,-16), (2,-18).
\]
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer:**

Perceptron update rule (learning rate = 1):

\[
w \leftarrow w + y \cdot x
\]

Starting from \(w_0 = (0, 0, 0, 0, 0)\), and using the fact that each example is updated `count` times, we add:

1. \((0,0,0,0,4)\), \(y=+1\), `count = 2`  
   Contribution: \(2 \cdot (+1) \cdot (0,0,0,0,4) = (0,0,0,0,8)\)

2. \((0,0,6,5,0)\), \(y=+1\), `count = 1`  
   Contribution: \((0,0,6,5,0)\)

3. \((3,0,0,0,0)\), \(y=-1\), `count = 1`  
   Contribution: \(-1 \cdot (3,0,0,0,0) = (-3,0,0,0,0)\)

4. \((0,9,3,6,0)\), \(y=-1\), `count = 1`  
   Contribution: \(-1 \cdot (0,9,3,6,0) = (0,-9,-3,-6,0)\)

5. \((0,1,0,2,5)\), \(y=-1\), `count = 1`  
   Contribution: \(-1 \cdot (0,1,0,2,5) = (0,-1,0,-2,-5)\)

Now sum all contributions coordinate-wise:

- First coord: \(0 + 0 - 3 + 0 + 0 = -3\)
- Second coord: \(0 + 0 + 0 - 9 - 1 = -10\)
- Third coord: \(0 + 6 + 0 - 3 + 0 = 3\)
- Fourth coord: \(0 + 5 + 0 - 6 - 2 = -3\)
- Fifth coord: \(8 + 0 + 0 + 0 - 5 = 3\)

So the final weight vector is:

\[
\boxed{w = (-3,\; -10,\; 3,\; -3,\; 3)}
\]
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer:**

Below I implement a simple Perceptron classifier on a small 2D dataset with
positive and negative examples. I plot the data points and then visualize the
decision boundary after each weight update to show how the Perceptron converges
to a stable linear separator.
Visualizing：
import numpy as np
import matplotlib.pyplot as plt

# 1. Construct a small 2D linearly separable dataset
# Positive points (label +1)
X_pos = np.array([
    [1, 2],
    [2, 3],
    [3, 3]
])

# Negative points (label -1)
X_neg = np.array([
    [2, 0],
    [3, 1],
    [4, 1]
])

X = np.vstack([X_pos, X_neg])         # shape (N, 2)
y = np.array([+1, +1, +1, -1, -1, -1])  # labels

# 2. Perceptron training
w = np.zeros(2)       # weight vector (no bias term for simplicity)
lr = 1.0              # learning rate
max_epochs = 20

# store weight vectors after each update for visualization
ws = [w.copy()]

for epoch in range(max_epochs):
    updates = 0
    for xi, yi in zip(X, y):
        # Perceptron prediction: sign(w · x)
        if yi * np.dot(w, xi) <= 0:
            # misclassified → update
            w = w + lr * yi * xi
            ws.append(w.copy())
            updates += 1
    if updates == 0:
        # converged: no misclassifications in this epoch
        break

print("Final weight:", w)
print("Number of updates:", len(ws) - 1)

# 3. Visualization of convergence
# We plot the data once, and draw a sequence of decision boundaries
# corresponding to each weight vector in ws.

# Create a grid of x-values for drawing lines
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
xs = np.linspace(x_min, x_max, 100)

plt.figure(figsize=(6, 6))

# plot points
plt.scatter(X_pos[:, 0], X_pos[:, 1], marker='o', label='Positive (+1)')
plt.scatter(X_neg[:, 0], X_neg[:, 1], marker='x', label='Negative (-1)')

# plot decision boundary after each update
for i, w_i in enumerate(ws):
    # w_i = (w1, w2), decision boundary: w1 * x1 + w2 * x2 = 0
    # If w2 != 0, x2 = -(w1 / w2) * x1
    if w_i[1] != 0:
        ys = -(w_i[0] / w_i[1]) * xs
        # earlier lines lighter, final line darker
        alpha = 0.2 + 0.8 * (i / max(1, len(ws) - 1))
        plt.plot(xs, ys, alpha=alpha)

plt.xlim(x_min, x_max)
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
plt.ylim(y_min, y_max)

plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Perceptron convergence: decision boundaries after each update')
plt.legend()
plt.grid(True)
plt.show()

"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
